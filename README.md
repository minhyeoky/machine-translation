# Neural Machine Translation

## Objectives
1. Make flexible deeplearning project template 
2. Compare performances of different models & methods
3. Understand and implement attention perfectly

## Models
- [x] Seq2seq
- [ ] Attention
- [ ] Transformer

## Visualization
- [x] BLEU [train, test]
- [x] Losses [train]
- [x] Times per step
- [x] Text examples [train, test]
  
## Principles
1. Well defined entry point for train & inference
2. From raw data to trained model
3. Notebooks are not allowed to push
4. All Configurations are controlled under json file
5. Use english
6. Test wherever possible

## Directory structure
```
ğŸ“¦nmt
 â”£ ğŸ“‚config
 â”£ ğŸ“‚data
 â”ƒ â”£ ğŸ“‚input
 â”ƒ â”— ğŸ“‚output
 â”£ ğŸ“‚script
 â”£ ğŸ“‚src
 â”ƒ â”£ ğŸ“‚data
 â”ƒ â”£ ğŸ“‚log
 â”ƒ â”£ ğŸ“‚model
 â”ƒ â”£ ğŸ“‚preprocessor
 â”ƒ â”£ ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“œconfig.py
 â”ƒ â”— ğŸ“œutils.py
 â”£ ğŸ“‚tests
 â”£ ğŸ“œ.gitignore
 â”£ ğŸ“œREADME.md
 â”£ ğŸ“œrequirements.txt
 â”£ ğŸ“œrun.sh
 â”— ğŸ“œtrain.py
```

## References
- [Tensorflow-tutorials](https://www.tensorflow.org/tutorials/text/nmt_with_attention)
- [huggingface](https://github.com/huggingface/transformers)