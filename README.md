# My NLP playground

## Models
- [x] Seq2seq
- [x] Attention
- [x] Transformer

## TODO
- [ ] run.sh 인자 파싱 파이썬에서 하기
- [ ] 설정파일 갈아 엎기
- [ ] 텐서플로 코드 테스트에 대해 알아보기
- [ ] GPT, XLNet
- [ ] generation 말고 다른 task
- [ ] 로깅 정리

## Visualization
- [x] BLEU [train, test]
- [x] Losses [train]
- [x] Times per step
- [x] Text examples [train, test]
- [ ] Number of parameters
- [ ] Visualize attention weights map
    - [ ] Bahdanau seq2seq
    - [ ] Transformer
  
## References
- [NMT by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf)
- [attention is all you need](https://arxiv.org/abs/1706.03762)
- [Tensorflow-tutorials](https://www.tensorflow.org/tutorials/text/nmt_with_attention)
- [huggingface](https://github.com/huggingface/transformers)
- [tensorflow-models](https://github.com/tensorflow/models)